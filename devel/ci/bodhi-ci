#!/usr/bin/python3
# Copyright Â© 2018 Red Hat, Inc.
#
# This file is part of Bodhi.
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Bodhi's CI command tool."""
import asyncio
import multiprocessing
import os
import signal
import subprocess
import sys
import uuid

import click


CONTAINER_NAME = 'bodhi-ci'
# We label the containers we run so it's easy to find them when we run _stop_all_jobs() at the end.
# UUID is used so that one bodhi-ci process does not stop jobs started by a different one.
CONTAINER_LABEL = 'purpose=bodhi-ci-{}'.format(uuid.uuid4())
PROJECT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
RELEASES = ('f27', 'f28', 'f29', 'rawhide', 'pip')


def _set_concurrency(ctx, param, value):
    """
    Set up the concurrency_semaphore.

    Args:
        ctx (click.core.Context): The Click context, unused.
        param (click.core.Option): The option being handled. Unused.
        value (str): The value of the -j flag.
    Returns:
        str: The value of the -j flag.
    """
    global concurrency_semaphore
    concurrency_semaphore = asyncio.Semaphore(value=value)
    return value


def _set_container_runtime(ctx, param, value):
    """
    Set up the container_runtime variable.

    Args:
        ctx (click.core.Context): The Click context, unused.
        param (click.core.Option): The option being handled. Unused.
        value (str): The value of the --container-runtime flag.
    Returns:
        str: The value of the --container-runtime flag.
    """
    global CONTAINER_NAME
    global container_runtime
    container_runtime = value
    if value == 'podman':
        # Workaround for https://github.com/containers/buildah/issues/1034
        CONTAINER_NAME = 'localhost/{}'.format(CONTAINER_NAME)
    return value


def _set_global(ctx, param, value):
    """
    Set up a global variable based on click input.

    Args:
        ctx (click.core.Context): The Click context, unused.
        param (click.core.Option): The option being handled. Used to find the global we are setting.
        value (str): The value of the flag.
    Returns:
        bool: The value of the flag.
    """
    globals()[param.name] = value
    return value


archive_option = click.option(
    '--archive', '-a', is_flag=True,
    help=("Collect *.xml from the tests and put them into test_results/."))
concurrency_option = click.option(
    '--concurrency', '-j', default=multiprocessing.cpu_count(), callback=_set_concurrency, type=int,
    help='Number of concurrent processes to run. Defaults to the number of cores detected')
container_runtime_option = click.option(
    '--container-runtime', '-c', default='docker', type=click.Choice(['docker', 'podman']),
    help='Select the container runtime to use. Defaults to docker.',
    callback=_set_container_runtime)
failfast_option = click.option('--failfast', '-x', is_flag=True,
                               help='Exit immediately upon error.', callback=_set_global)
init_option = click.option('--init/--no-init', default=True,
                           help="Use the container runtime's --init flag.", callback=_set_global)
no_build_option = click.option(
    '--no-build', is_flag=True,
    help='Do not run docker build if the image already exists.')
pyver_option = click.option(
    '--pyver', '-p', default=[2, 3], multiple=True, type=int,
    help=("Limit to a particular Python major version. May be specified multiple times. "
          "Acceptable values: 2, 3"))
releases_option = click.option(
    '--release', '-r', default=list(RELEASES), multiple=True,
    help=("Limit to a particular release. May be specified multiple times. "
          "Acceptable values: {}".format(', '.join(RELEASES))))
tty_option = click.option('--tty/--no-tty', default=True, help='Allocate a pseudo-TTY.',
                          callback=_set_global)

# These command maps define how to run each type of test, how to label them in the output,
# and overrides for how to run them that are release specific (pip sometimes uses different
# executables.)
docs_command_map = {
    'label': 'docs',
    'default': [
        '/usr/bin/bash', '-c',
        ('/usr/bin/python2 setup.py develop && make -C docs clean && make -C docs html && make '
         '-C docs man')]}
flake8_command_map = {
    'label': 'flake8',
    'default': '/usr/bin/flake8',
    'pip': '/usr/local/bin/flake8'}
pydocstyle_command_map = {
    'label': 'pydocstyle',
    'default': ['/usr/bin/pydocstyle', 'bodhi'],
    'pip': ['/usr/local/bin/pydocstyle', 'bodhi']}

concurrency_semaphore = None
container_runtime = None
failfast = False
init = True
tty = False


@click.group()
def cli():
    """
    Bodhi's Continuous Integration helper script.
    """


@cli.command()
@archive_option
@concurrency_option
@container_runtime_option
@failfast_option
@init_option
@no_build_option
@pyver_option
@releases_option
@tty_option
def all(archive, concurrency, container_runtime, no_build, failfast, init, pyver, release,
        tty):
    """Run all the types of tests in parallel."""
    command_maps = [docs_command_map, flake8_command_map, pydocstyle_command_map]
    command_maps.extend(_generate_unit_command_maps(pyver))

    _container_run(no_build, release, command_maps, archive)


@cli.command()
@concurrency_option
@container_runtime_option
@failfast_option
@releases_option
@tty_option
def build(concurrency, container_runtime, failfast, release, tty):
    """Build the containers for testing."""
    _build(release)


@cli.command()
@concurrency_option
@container_runtime_option
@init_option
@releases_option
@tty_option
def clean(concurrency, container_runtime, init, release, tty):
    """Remove all builds pertaining to Bodhi CI."""
    jobs = {}
    for r in release:
        args = [container_runtime, 'rmi',
                '{}/{}'.format(CONTAINER_NAME, r)]
        jobs['{:>8}'.format(r)] = args

    _run_processes(jobs)


@cli.command()
@concurrency_option
@container_runtime_option
@failfast_option
@init_option
@no_build_option
@releases_option
@tty_option
def docs(concurrency, container_runtime, failfast, init, no_build, release, tty):
    """Build the docs."""
    _container_run(no_build, release, docs_command_map)


@cli.command()
@concurrency_option
@container_runtime_option
@failfast_option
@init_option
@no_build_option
@releases_option
@tty_option
def flake8(concurrency, container_runtime, failfast, init, no_build, release, tty):
    """Run flake8 tests."""
    _container_run(no_build, release, flake8_command_map)


@cli.command()
@concurrency_option
@container_runtime_option
@failfast_option
@init_option
@no_build_option
@releases_option
@tty_option
def pydocstyle(concurrency, container_runtime, failfast, init, no_build, release, tty):
    """Run pydocstyle tests."""
    _container_run(no_build, release, pydocstyle_command_map)


@cli.command()
@archive_option
@concurrency_option
@container_runtime_option
@failfast_option
@init_option
@no_build_option
@pyver_option
@releases_option
@tty_option
def unit(archive, concurrency, container_runtime, no_build, failfast, init, pyver, release, tty):
    """Run the unit tests."""
    _container_run(no_build, release, _generate_unit_command_maps(pyver), archive)


def _build(releases):
    """
    Build the container images needed to test the given releases.

    Args:
        releases (list): A list of strings mapping to releases to build container images for.
    """
    jobs = {}
    for r in releases:
        dockerfile = os.path.join(PROJECT_PATH, 'devel', 'ci', 'Dockerfile-{}'.format(r))
        args = [container_runtime, 'build', '--pull', '-t', '{}/{}'.format(CONTAINER_NAME, r), '-f',
                dockerfile, '.']
        jobs['{:>8}'.format(r)] = args

    _run_processes(jobs)


def _container_run(no_build, releases, command_maps, archive=False):
    """
    Run the commands described by the given command_maps in parallel.

    A command_map is a dictionary that provides a label describing a job, a mapping from release
    name to a job description, and a default job description for releases not described. For
    example, the following will note the job as 'some-cool-job' when reporting on the terminal,
    has a default executable job to run, and has an override for the job to run for the pip release:

        {'label': 'some-cool-job',
         'default': '/usr/bin/neat_executable',
         'pip': '/usr/local/bin/neat_thing_over_here'}

    The commands referenced by the 'default' or release override keys can also be expressed as lists
    suitable for passing to subprocess.Popen().

    Args:
        no_build (bool): If True, ensure the builds exist and only build them if they do
            not. If False, build them.
        releases (list): A list of strings describing the releases to run the given jobs for.
        command_maps (list): A list of dictionaries describing commands to be run. See above for
            a description of the dictionary.
    """
    if no_build:
        _ensure_builds_exist(releases)
    else:
        _build(releases)

    if isinstance(command_maps, dict):
        command_maps = [command_maps]

    jobs = {}
    for command_map in command_maps:
        for r in releases:
            if r in command_map:
                command = command_map[r]
            else:
                command = command_map['default']

            if isinstance(command, str):
                command = [command]

            args = [container_runtime, 'run', '--network', 'none', '--rm',
                    '--label', CONTAINER_LABEL]

            if init:
                args.append('--init')

            if tty:
                args.append('-t')

            if archive:
                archive_dir = '{}/test_results/{}'.format(
                    PROJECT_PATH, '{}-{}'.format(r, command_map['label']))
                args.extend(['-v', '{}:/results:z'.format(archive_dir)])

            args.append('{}/{}'.format(CONTAINER_NAME, r))
            args.extend(command)

            jobs['{:>8}-{:<12}'.format(r, command_map['label'])] = args

    _run_processes(jobs)


def _ensure_builds_exist(releases):
    """
    Ensure that container images exist for the given releases.

    Any container images that don't exist will be built.

    Args:
        releases(list): A list of strings naming releases.
    """
    releases_to_build = set()
    for r in releases:
        expected_repository = '{}/{}'.format(CONTAINER_NAME, r)
        args = [container_runtime, 'images', expected_repository]
        images = subprocess.check_output(args).decode()
        if expected_repository not in images:
            releases_to_build.add(r)
    if releases_to_build:
        _build(releases_to_build)


def _format_output(output, label):
    """
    Run decode on the given output, and then prepend label in front of each line.

    Args:
        output (bytes): The output from Process.communicate().
        label (str): The label to prepend on each line.
    """
    if not output:
        return ''
    output = output.decode()
    return '\n'.join(['{}\t{}'.format(label, line) for line in output.split('\n')])


def _process_results(loop, done, pending):
    """
    Process the finished and pendings tasks and return error output, a summary, and an exit code.

    This function is used by _run_processes() to generate the final stdout to be printed (which is
    going to be the output of the failed tasks since the cancelled tasks and successful tasks
    already had their output printed), a summary block for humans to read, and an exit code that
    bodhi-ci should use. Any pending tasks will be cancelled.

    Args:
        loop (asyncio.AbstractEventLoop): The event loop. This is used to cancel any pending tasks.
        done (set): A set of asyncio.Tasks that represent finished tasks.
        pending (set): A set of asyncio.Tasks that represent unfinshed tasks. These will be
            canceled.
    Returns:
        dict: A dictionary with three keys:
            'summary': Indexing a str summarizing the jobs.
            'output': The output that should be printed.
            'returncode': The exit code that bodhi-ci should exit with.
    """
    returncode = 0
    error_output = ''
    summary = ''

    if pending:
        for task in pending:
            task.cancel()
        future = asyncio.wait(pending)
        cancelled, pending = loop.run_until_complete(future)
        done = done | cancelled
        returncode = -signal.SIGINT

    for task in done:
        try:
            result = task.result()
        except RuntimeError as e:
            # This task failed, or was cancelled.
            result = e.result
            if result['stdout']:
                if result['returncode'] < 0:
                    # This was canceled, so let's print it now so the error output can go
                    # last.
                    click.echo(result['stdout'])
                else:
                    error_output = '{}\n{}'.format(error_output, result['stdout'])
            if not returncode:
                returncode = result['returncode']
        summary = summary + result['summary']

    # Let's sort the summary lexicographically so releases show near each other.
    summary = '\n'.join(sorted([l for l in summary.split('\n')]))

    return {'summary': summary, 'output': error_output, 'returncode': returncode}


def _generate_result(job, returncode, output):
    """
    Generate the result format that _run_process() needs to return.

    Args:
        job (str): The job label.
        returncode (int): The process exit code.
        output (bytes): The output from process.communicate().
    Returns:
        dict: A dictionary with the following keys:
            job (str): A label identifying the job.
            summary (str): A summary line to show the user about this job at the end.
            returncode (int): The process' exit code.
            stdout (str): The process' stdout.
    """
    return {
        'job': job, 'returncode': returncode,
        'summary': _summary_line(returncode, job), 'stdout': _format_output(output, job)}


def _generate_unit_command_maps(pyvers):
    """
    Return a list of command maps suitable for _container_run() that run the unit tests.

    Args:
        pyvers (list): A list of integers for which Python versions to test. Only 2 and 3 are used.
    Retutns:
        list: A list of dictionaries (known as command maps - see the help for _container_run() for
            a description of the schema.)
    """
    pytest_flags = ''
    if failfast:
        pytest_flags = '-x'

    test_command = ('({} setup.py develop && {} {} || (cp *.xml /results && exit 1)) '
                    '&& cp *.xml /results')
    py2_command_map = {
        'label': 'python2-unit',
        'default': [
            '/usr/bin/bash', '-c', test_command.format(
                '/usr/bin/python2', '/usr/bin/py.test-2', pytest_flags)],
        'pip': [
            '/usr/bin/bash', '-c', test_command.format(
                '/usr/bin/python2', '/usr/bin/py.test', pytest_flags)]}
    py3_command_map = {
        'label': 'python3-unit',
        'default': [
            '/usr/bin/bash', '-c', test_command.format(
                '/usr/bin/python3', '/usr/bin/py.test-3', pytest_flags)],
        'pip': [
            '/usr/bin/bash', '-c', test_command.format(
                '/usr/bin/python3', '/usr/local/bin/py.test', pytest_flags)]}

    command_maps = []
    if 2 in pyvers:
        command_maps.append(py2_command_map)
    if 3 in pyvers:
        command_maps.append(py3_command_map)

    return command_maps


async def _run_process(job, *args, **kwargs):
    """
    Run a subprocess, returning its label, summary, return code, and stdout.

    Cancelled processes will have their return code set to -2, no matter what their real return code
    was. This makes it easy to identify cancelled processes.

    Args:
        job (str): The label to prepend on all stdout for the job.
        args (list): A list of args to pass to asyncio.create_subprocess_exec().
        kwargs (dict): The keyword arguments to pass to asyncio.create_subprocess_exec().
    Returns:
        dict: A dictionary with the following keys:
            job (str): A label identifying the job.
            summary (str): A summary line to show the user about this job at the end.
            returncode (int): The process' exit code, or -2 if it was cancelled.
            stdout (str): The process' stdout.
    """
    cancelled = False
    stdout = b''

    async with concurrency_semaphore:
        process = await asyncio.create_subprocess_exec(*args, **kwargs)

        try:
            stdout, stderr = await process.communicate()
        except asyncio.CancelledError as e:
            try:
                process.terminate()
            except ProcessLookupError:
                # The process is already stopped, nothing to see here.
                pass
            cancelled_stdout, stderr = await process.communicate()
            stdout = stdout + cancelled_stdout
            cancelled = True

    returncode = -2 if cancelled else process.returncode

    result = _generate_result(job, returncode, stdout)

    # If the job's been cancelled or successful, let's go ahead and print its output now. Failed
    # jobs will have their output printed at the end.
    if returncode <= 0 and result['stdout']:
        click.echo(result['stdout'])

    if process.returncode != 0:
        error = RuntimeError()
        error.result = result
        raise error

    return result


def _run_processes(jobs):
    """
    Run the given jobs in parallel.

    Start a process for each job in the jobs map. The stdout and stderr for each process is
    written to the terminal. Processes that exited with code 0 are output first, followed by any
    processes that failed. Lastly, a summary report for the jobs is printed, indicating success or
    failure for each one. If any jobs failed, one of the failed jobs' exit code will be used to exit
    this process.

    Args:
        jobs (dict): A dictionary mapping job labels (str) to a list of strings which are suitable
            as arguments to subprocess.Popen().
    """
    popen_kwargs = {'shell': False}
    if len(jobs) > 1:
        # If there's more than one job, let's buffer the output so the user doesn't see a jumbled
        # mess.
        popen_kwargs['stdout'] = subprocess.PIPE
        popen_kwargs['stderr'] = subprocess.STDOUT

    loop = asyncio.get_event_loop()

    [click.echo('Running {}'.format(' '.join(args))) for j, args in jobs.items()]
    processes = [_run_process(j, *args, **popen_kwargs) for j, args in jobs.items()]

    return_when = asyncio.ALL_COMPLETED
    if failfast:
        return_when = asyncio.FIRST_EXCEPTION
    future = asyncio.wait(processes, return_when=return_when)
    # Catch SIGINT, but don't do anything. We just don't want the signal to cause the Exception to
    # bubble up out of the main loop.
    loop.add_signal_handler(signal.SIGINT, lambda: None)

    try:
        done, pending = loop.run_until_complete(future)

        results = _process_results(loop, done, pending)
    finally:
        _stop_all_jobs(loop)

    # Now it's time to print any error output we collected and the summary, then exit or return.
    click.echo(results['output'])
    click.echo('\n\n{}'.format(results['summary']))
    if results['returncode']:
        sys.exit(results['returncode'])


def _stop_all_jobs(loop):
    """
    Stop all running docker jobs with the CONTAINER_LABEL.

    Even though we terminate() all of our child processes above, Docker does not always proxy
    signals through to the container, so we will do a final cleanup to make sure all the jobs we
    started in this process have been told to stop.

    Args:
        loop (asyncio.AbstractEventLoop): The event loop.
    """
    args = [container_runtime, 'ps', '--filter=label={}'.format(CONTAINER_LABEL), '-q']
    processes = subprocess.check_output(args).decode()
    jobs = {}
    for process in processes.split('\n'):
        if process:
            jobs[process] = [container_runtime, 'stop', process]

    # If you give run_until_complete a future with no tasks, you will haz a sad (that's the
    # technical wording for a ValueError).
    if jobs:
        stop_processes = [_run_process(j, *args, stdout=subprocess.DEVNULL, shell=False)
                          for j, args in jobs.items()]
        stop_future = asyncio.wait(stop_processes)
        loop.run_until_complete(stop_future)


def _summary_line(exit_code, job):
    """
    Create a summary line for the given job with given exit_code.

    If the exit_code indicates failure, it is printed to the console immediately. Failed jobs'
    stdout is not printed until the end of the job, so this gives the user a way to know that a job
    failed before its output is printed, and they can ctrl-c to see its output.

    Args:
        exit_code (int): The exit code of the given job - used to determine which color/message to
            print.
        job (str): The label for the job.
    Returns:
        str: A summary line suitable to print at the end of the process.
    """
    if exit_code == 0:
        color_start = '\033[0;32m' if tty else ''
        color_end = '\033[0m' if tty else ''
        summary_line = '{}:  {}SUCCESS!{}\n'.format(job, color_start, color_end)
    elif exit_code < 0:
        color_start = '\033[0;33m' if tty else ''
        color_end = '\033[0m' if tty else ''
        summary_line = '{}:  {}CANCELED{}\n'.format(
            job, color_start, color_end)
    else:
        color_start = '\033[0;31m' if tty else ''
        color_end = '\033[0m' if tty else ''
        summary_line = '{}:  {}FAILED{}  (exited with code: {})\n'.format(
            job, color_start, color_end, exit_code)
        click.echo(summary_line)

    return summary_line


cli()
